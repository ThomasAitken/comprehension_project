Adjectives can be 'objective' or 'perspectival'...

Perspectival = 'delicious', 'revolting', 'awesome'

Objective = 'large', 'hairy', 'smelly' [ambiguous]

Lots of edge cases... everything a matter of perception... Probabilistic models




Challenge: how can we take simple descriptive paragraph capturing a scene, say, inside a room, and turn it into a model such that questions can be asked about which objects are on which surfaces, which things would break if they fell over, what path would a human take to navigate around the objects, and so on. Obviously that requires a 'visual'-style representation (i.e. a representation involving quantities - geometry, physics, i.e. similar to game engine sort of representation). BUT hopefully this kind of project could be hooked up to such a representation. 

Idea for initial step towards this extension of representation: create generic models for basic 'settings/locations' and basic objects/entities that can be placed in such settings. Then every time certain words/concepts occur in text, those models are brought into memory for reference == richer model.

Long-term plan for learning rich representations: 

    (i) first take text-based reasoning as far as possible, similar to this project; 
    (ii) run deep learning system inside rich virtual world ('Second Life'-style) edited so that everything is labelled according to its category (down to furniture, etc (already happens to a large extent in a lot of RPGs, at least as far as portable items are concerned)); with 100s or 1000s of hours of immersion in world, the system will ultimately accumulate 100s or 1000s of samples of each category from lots of different angles... Hopefully system then able to recognise new instances of such categories that are similar from multiple angles.

In the meantime, need to develop further data/memory systems. An event/agent log... Most conceptually basic representation of such data I can think of (very bulky) is some kind of JSON dict where main key is the agent code, and then this links to another dictionary with keys "properties" (list) and "events" (which happens to encode all the state transitions the agent has undergone and the last known state). 

Then also specific entity memory... Because of course concept memory is not enough; we want rich memories associated with specific buildings, tools, games, landmarks, places, etc. ... Now this may be not best implemented in this kind of text-based context.

These could be very neatly compartmentalised in theory, although this would of course come with massive redundancy - but, frankly, static memory capacity doesn't seem like a huge theoretical bottleneck.. Of course the memory storage that would be going on after 1000s of hours of training in a rich virtual world where events are constantly being perceived, new properties gleaned from objects, and so on\



Need to implement knowledge of Maslow's Hierarchy of Needs in order to make inferences about motivations, e.g. Why "eat"? Because "hungry" because basic need. Why "go outside for a walk"? Can this be reduced ultimately to some kind of need? If so...

identify "for" as a substitute for "in order to do"; identify "to" as a subsitute for "in order to". USe semantic similarity to classify ultimate purpose in question in terms of Maslow Hierarchy. 


BOOTSTRAP DEEP LEARNING OF ENTIRE COMMONSENSE MODELS OF THE WORLD IN RELATION TO SPECIFIC CAUSAL CONTEXTS... CREATE DATASETS with sentnece/event data with event whose cause needs to be explained + possible cause-candidate events.. Classify which event is cause.

The ultimate way to acquire a commonsense model of the world? 


Phrases of form: 'The X of the Y' give direct information about kind of properties that Y can have = valuable info in world-model.
Similarly: 'Its X' or 'Her Y'. 


TWO DATABASES:
First DB stores concepts and associated meaning vectors in terms of universal category terms.. 
    Do we continuously 'calibrate' concepts as follows? :
        Initial vector = average of phrase-level and atomic vectors. 
        If same word appears then test similarity to ave of phrase-level/atomic vectors for word in new context... Compute difference and decide if bifurcation needs to occur. So some threshold, but arbitrary ... Work manually initially until a threshold can be determined by classification... Complex data pipeline. (Or decision rule = IF top category = top category and {cat_2, cat_3} = {cat_2, cat_3} then calibrate ELSE bifurcate)

Entity information:
    People/characters: [associated descriptors (w/ count of frequency for each, to measure strength of association),  known stable properties, key events (Maslow hierarchy --> self-actualisation???)]
    Objects/living beings/other: //

    Associated descriptors, if sufficiently strongly associated, essentially become known stable properties associated with all members of the class... e.g. it's a fair indication that if 'creepy' always paired with cockroaches then that is a property that is (strongly) associated with all cockroaches, not with a subset.


    THOUGHT: Need to use Wolfram knowledge-base as base... https://products.wolframalpha.com/summary-boxes-api/documentation/. Exciting thought that I don't need to bootstrap.




STEPHEN WOLFRAM on history of symbolic AI/expert systems and position of Wolfram Language therein (interview with Lex Fridman, timestamp 2:23):
    Paraphrase: "Problem with historic expert systems not ambitious enough. Can't do natural-language understanding properly within domain unless you take on the whole problem of building a system that understands the world in general. This is what Wolfram Language did." [[[editorial: because human language is intertwined with human understanding of the world and to interpret even a single NL sentence properly you typically need to draw on some subset of rich common-sense models of space, time, physics, human behaviour, emotions, etc, and more 'static' knowledge about specific entities and their properties]]].

    Agree! My first model of the world is entirely represented in the Spacy Word2Vec model and in the rules I'm encoding to maximally exploit it. But with more data-collection and training I can hopefully 





